---

# Defaults for ansible-role-slurm
#

# Packages installed are defined in vars/slurm_*

admingroup: "admin"

# Overwrite DB_root_password in your vars file if you want to 
# use a different password for root and slurm user
DB_root_password: "{{ slurm_mysql_password }}"
# Disable mysql security tasks by setting slurm_manage_mysql_security to False
slurm_manage_mysql_security: True

# If the current node is a nis server, make sure the slurm user and
# group exist
nis_server: False

# option to manually force slurm_user_uid regardless of nis server
#slurm_user_uid: 5004
#slurm_user_gid: 5004

slurm_repo: "ohpc"  # Or "ohpc" to use OHPC slurm packages
slurm_ohpc_versionlock: True
siteName: "io"
nodeBase: "{{ siteName }}"
nodeGpuBase: "gpu"

# this role has been written to support slurm >=18.08. Some conf parameters have been deprecated from newer Slurm versions so this is for backward compatibility
slurm_major_version: 18

# Have systemd restart slurm daemons if they fail to start
slurm_systemd_override_slurmd: True
slurm_systemd_override_slurmdbd: True
slurm_systemd_override_slurmctld: True
slurm_systemd_override_restart: "on-failure"
slurm_systemd_override_sec: "20"

# By setting slurm_munge_key_from_nfs to True we copy the munge.key from slurm_munge_key_nfs
# If it's False then we copy it from files/ where ansible runs
# This is the default. So with ansible-pull we set slurm_munge_key_nfs to True
slurm_munge_key_from_nfs: False

# By default we copy the munge.key to NFS
slurm_munge_key_to_nfs: True
slurm_munge_key_nfs_dir: "/home/{{ admingroup }}"
slurm_munge_key_nfs: "{{ slurm_munge_key_nfs_dir }}/munge.key"

slurm_SlurmUser: "slurm"
slurm_SlurmctldPort: "6817"
slurm_SlurmdPort: "6818"
slurm_AuthType: "auth/munge"
slurm_SlurmctldPidFile: "/var/run/slurmctld.pid"
slurm_SlurmdbdPidFile: "/var/run/slurmdbd.pid"
slurm_SlurmdPidFile: "/var/run/slurmd.pid"
slurm_CacheGroups: "1"
slurm_Prolog: "/usr/local/libexec/slurm/prolog.d/*"
slurm_Epilog: "/usr/local/libexec/slurm/epilog.d/*"
slurm_TaskEpilog: "/usr/bin/epilog"
slurm_TaskPlugin: "task/affinity,task/cgroup"
slurm_log_dir: "/var/log/slurm"
slurm_logrotate_rotate_interval: "weekly"
slurm_logrotate_rotate: "8"
slurm_logrotate_signal: "-HUP"
slurm_state_dir: "/opt/slurmdb/tmp"

slurm_switch_type: "switch/none"
slurm_mpi_default: "none"
slurm_mpi_params: "ports=12000-12999"
slurm_proctrack_type: "proctrack/cgroup"
slurm_service_node: "{{ groups['slurm_service'][0] }}"

#slurm_control_addr is the same as ControlMachine ( slurm_service_node ) by default. 
#Define it here to change it.
#slurm_control_addr="{{ slurm_service_node }}"
# If you want a backup slurmctld, set this variable.
#slurm_backup_controller: "{{ groups['slurm_service'][1] }}"
# The addr of the backup controller, if necessary
#slurm_backup_addr: "foo.example.com"

slurm_accounting_storage_host: "{{ slurm_service_node }}"
slurm_accounting_storage_type: "accounting_storage/slurmdbd"
slurm_accounting_storage_loc: "slurm_acct_db"
slurm_accounting_storage_user: "slurm"
slurm_accounting_storage_enforce: "safe,qos"
# If you wish to add accounting for additional TRES types, uncomment and edit the following:
#slurm_accounting_storage_tres: "gres/gpu:teslak80,gres/gpu:teslak40"

# slurm_AccountingStorageTRES is used if slurm_with_gpu is True
slurm_AccountingStorageTRES: "gres/gpu"


##

# If you wish to use a topology plugins (parameter TopologyPlugin=),
# uncomment and edit the following:
#slurm_topology_plugin: "topology/tree"
# If you use a topology plugin, you also need a topology.conf
# defined here, one element per line
#slurm_topologylist:
#  - "SwitchName=s0 Nodes=c[0-18]"
#  - "SwitchName=s1 Nodes=c[19-36]"
#  - "SwitchName=s3 Switches=s[0,1]"
slurm_clustername: "{{ siteName | default('test_cluster')}}"
slurm_compute_nodes: "{{ nodeBase }}[1-4]"
slurm_compute_realmemory: "126000"
slurm_compute_sockets: "2"
slurm_compute_corespersocket: "12"
slurm_compute_threadspercore: "2"
slurm_node_state: "UNKNOWN"
slurm_with_gpu: True
slurm_gpu_nodes: "{{ nodeGpuBase }}"
slurm_grestypes: "gpu"
slurm_gpu_type: "teslak80"
slurm_gpu_count: "8"
slurm_greslist:
  - "NodeName={{ nodeGpuBase }} Name=gpu Type={{ slurm_gpu_type }} File=/dev/nvidia[0-{{ slurm_gpu_count|int - 1 }}]"

# TIMERS
slurm_SlurmctldTimeout: "300"
slurm_SlurmdTimeout: "600"
slurm_InactiveLimit: "0"
slurm_MinJobAge: "30"
slurm_MessageTimeout: "30"
slurm_KillWait: "30"
slurm_CompleteWait: "12"
slurm_Waittime: "0"
slurm_KillOnBadExit: "1"
slurm_KeepAliveTime: "60"
slurm_OverTimeLimit: "60"

##
slurm_first_job_id: "2230000"
slurm_return_to_service: "1"
slurm_max_job_count: "30000"
slurm_max_job_time: "7-00:00:00"
slurm_job_def_time: "23:59:59"
slurm_max_array_size: "10001"

slurm_propagate_resource_limits_except: "MEMLOCK"
slurm_enforce_part_limits: "YES"

slurm_usepam: "1"
slurm_RebootProgram: "/sbin/reboot"
slurm_communication_parameters: "block_null_hash,keepalivetime={{ slurm_KeepAliveTime }}"


# If you wish to use job submit plugins (parameter JobSubmitPlugins=),
# uncomment and edit the following:
# slurm_jobsubmitplugins: "lua"

slurm_SchedulerType: "sched/backfill"
slurm_scheduler_parameters: "bf_max_job_test=300,bf_max_job_part=200,bf_max_job_user=30,defer,bf_continue,bf_window=7200,bf_resolution=1800,pack_serial_at_end"

slurm_select_type: "select/cons_res"
slurm_select_type_parameters: "CR_Core_Memory"
slurm_fast_schedule: "2"
slurm_priority_type: "priority/multifactor"
slurm_priority_flags: "FAIR_TREE,SMALL_RELATIVE_TO_TIME,MAX_TRES"
slurm_priority_decayhalflife: "14-0"
slurm_priority_favorsmall: "NO"
slurm_priority_weight_fairshare: "10000000"
slurm_priority_weight_age: "10000"
slurm_priority_weight_partition: "0"
slurm_priority_weight_jobsize: "10000"
slurm_priority_weight_qos: "10000"
slurm_priority_maxage: "7"
# to not have any HealthCheckProgram set slurm_healthcheck_program: "" 
slurm_healthcheck_program: "/usr/sbin/nhc"
slurm_healthcheck_interval: "300"
slurm_cgroup_mountpoint: "/sys/fs/cgroup"
slurm_cgroup_automount : "no"
slurm_cgroup_constrain_cores: "yes"
slurm_cgroup_constrain_ram: "no" # Use the very strict cgroup-based memory tracker?
slurm_cgroup_allowedramspace: "100.1" # At which % of requested memory should the process get killed?
slurm_cgroup_constrain_swap: "no"
slurm_cgroup_taskaffinity: "no"
slurm_cgroup_allowedswapspace: "120" # % of memory allocation that can be phys mem+swap
slurm_cgroup_constrain_devices: "no" 
slurm_cgroup_min_ram: "500" # always allocate this much memory to each job

# LOGGING
slurm_SlurmctldDebug: "3"
slurm_slurmctld_syslog_debug: "info"
slurm_SlurmdDebug: "3"
slurm_slurmd_syslog_debug: "info"
slurm_JobCompLoc: "{{ slurm_log_dir }}/slurm_jobcomp.log"
slurm_JobCompType: "jobcomp/filetxt"
slurm_slurmdbd_syslog_debug: "info"

# ACCOUNTING
slurm_JobAcctGatherType: "jobacct_gather/linux"
slurm_JobAcctGatherFrequency: "energy=60,task=60"
slurm_AcctGatherEnergyType: "acct_gather_energy/rapl"
slurm_AcctGatherNodeFreq: "60"
slurm_accounting_store_flags: "job_comment"

# Job-private /tmp, /var/tmp and /dev/shm
pam_use_namespace: False
pam_tmp_inst_dir: /l/tmp-inst/
pam_var_tmp_inst_dir: /l/vartmp_inst/

# template in a file to manage slurm logs via rsyslog
slurm_manage_rsyslog_conf: True
# sysctl settings from slurm htc guide.
slurm_manage_sysctl: True
slurm_sysctl_core_somaxconn: "4096" # 128 is the default in el7
slurm_sysctl_tcp_max_syn_backlog: "4096" # 512 is the default in el7

#slurm_nodelist:
# - "NodeName=cn[1-1234] RealMemory=128000 Feature=haswell"
# - "NodeName=gpu[1-1234] RealMemory=128000 Feature=haswell"

#slurm_partitionlist:
# - "PartitionName=batch Nodes={{ slurm_compute_nodes }} Default=NO MaxTime=INFINITE State=UP DefaultTime=2:00:00"
# - "PartitionName=grid Nodes=gpu[1-1234] Default=NO MaxTime={{ slurm_max_job_time }} State=UP DefaultTime=2:00:00"

# To add extra parameters, uncomment the following line and the desired parameter's lines. Add any other absent but needed parameter to the list slurm_ExtraParamsList. Entries in the list needs to be in Setting=value like below.
#slurm_ExtraParamsList:
# - JobCredentialPrivateKey=""
# - JobCredentialPublicCertificate=""
# - PluginDir=""
# - PlugStackConfig=""
# - PropagatePrioProcess=""
# - PropagateResourceLimits=""
# - PrologSlurmctld="/etc/slurm/slurmctld_prolog"
# - EpilogSlurmctld="/etc/slurm/slurmctld_epilog"
# - SrunProlog=""
# - SrunEpilog=""
# - TaskProlog="/usr/local/libexec/slurm/taskprolog.sh" 
# - TrackWCKey="no"
# - TreeWidth="50"
# - TmpFs=""
# - SchedulerAuth=""
# - SchedulerRootFilter=""
# - DefMemPerCPU="512"
# - PriorityUsageResetPeriod="14-0"
# - HealthCheckNodeState="IDLE"
# - Licenses="mdcs:256"
# - MaxSubmitJobs="2000"
# - SlurmctldLogFile={{ slurm_log_dir }}"/slurmctld.log"
# - DebugFlags="backfill"
# - SlurmdLogFile={{ slurm_log_dir }}"/slurmd.log"
# - JobAcctGatherFrequency="task=30"
# - AccountingStoragePass=""

# plugstack and X11
slurm_plugstack: False
slurm_x11_spank: False
slurm_x11_setting: "#optional          x11.so"

slurm_spank_x11_packages:
  - xauth
  - slurm-spank-x11

# default for apt repository if using this for ubuntu clients
slurm_apt_repo: False
slurm_apt_url: "http://localhost/apt"

slurm_taskprolog_script: "empty_taskprolog.sh.j2"
slurm_taskepilog_script: "empty_taskepilog.sh.j2"
slurm_prolog_scripts:
  - "empty_prolog.sh.j2"
slurm_epilog_scripts:
  - "namespace_clean.sh.j2"
