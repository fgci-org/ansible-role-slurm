---

# Defaults for ansible-role-slurm
#

# Packages installed everywhere
slurm_packages:
 - slurm
 - slurm-devel
 - slurm-perlapi
 - slurm-pam_slurm
 - slurm-sjstat
 - slurm-torque
 - slurm-munge
 - slurm-sjobexit
# - slurm-spank-x11
#

# Additional packages installed only on slurmctld node(s)
slurm_service_packages:
  - lua-devel
  - mailx
  - slurm-lua
  - slurm-plugins
  - slurm-slurmdb-direct
  - slurm-sql

admingroup: "admin"

# Overwrite DB_root_password in your vars file if you want to 
# use a different password for root and slurm user
DB_root_password: "{{ slurm_mysql_password }}"

# If the current node is a nis server, make sure the slurm user and
# group exist
nis_server: False

fgci_install: True
fgci_slurmrepo_version: "fgcislurm1508"
siteName: "io"
nodeBase: "{{ siteName }}"
nodeGpuBase: "gpu"

# By setting slurm_munge_key_from_nfs to True we copy the munge.key from slurm_munge_key_nfs
# If it's False then we copy it from files/ where ansible runs
# This is the default. So with ansible-pull we set slurm_munge_key_nfs to True
slurm_munge_key_from_nfs: False

# By default we copy the munge.key to NFS
slurm_munge_key_to_nfs: True
slurm_munge_key_nfs_dir: "/home/{{ admingroup }}"
slurm_munge_key_nfs: "{{ slurm_munge_key_nfs_dir }}/munge.key"

slurm_SlurmUser: "slurm"
slurm_SlurmctldPort: "6817"
slurm_SlurmdPort: "6818"
slurm_AuthType: "auth/munge"
slurm_SlurmctldPidFile: "/var/run/slurmctld.pid"
slurm_SlurmdPidFile: "/var/run/slurmd.pid"
slurm_CacheGroups: "1"
slurm_Prolog: "/etc/slurm/prolog"
slurm_Epilog: "/usr/local/libexec/slurm/epilog.d/*"
slurm_TaskEpilog: "/usr/bin/epilog"
slurm_TaskPlugin: "task/cgroup"
slurm_log_dir: "/var/log/slurm"
slurm_logrotate_rotate_interval: "weekly"
slurm_logrotate_rotate: "8"
slurm_tmp_dir: "/opt/slurmdb/tmp/slurmd"
slurmd_tmp_dir: "{{ slurm_tmp_dir }}"
slurm_state_dir: "/opt/slurmdb/tmp"

slurm_switch_type: "switch/none"
slurm_mpi_default: "none"
slurm_mpi_params: "ports=12000-12999"
slurm_proctrack_type: "proctrack/cgroup"
slurm_service_node: "{{ hostvars[groups['slurm_service'][0]]['ansible_hostname']  }}"
slurm_accounting_storage_host: "{{ slurm_service_node }}"
slurm_accounting_storage_type: "accounting_storage/slurmdbd"
slurm_accounting_storage_loc: "slurm_acct_db"
slurm_accounting_storage_user: "slurm"
slurm_accounting_storage_enforce: "safe,qos"
##

# If you wish to use a topology plugins (parameter TopologyPlugin=),
# uncomment and edit the following:
#slurm_topology_plugin: "topology/tree"
# If you use a topology plugin, you also need a topology.conf
# defined here, one element per line
#slurm_topologylist:
#  - "SwitchName=s0 Nodes=c[0-18]"
#  - "SwitchName=s1 Nodes=c[19-36]"
#  - "SwitchName=s3 Switches=s[0,1]"
slurm_clustername: "{{ siteName | default('test_cluster')}}"
slurm_compute_nodes: "{{ nodeBase }}[1-4]"
slurm_compute_realmemory: "126000"
slurm_compute_sockets: "2"
slurm_compute_corespersocket: "12"
slurm_compute_threadspercore: "2"
slurm_node_state: "UNKNOWN"
slurm_with_gpu: True
slurm_gpu_nodes: "{{ nodeGpuBase }}"
slurm_grestypes: "gpu"
slurm_greslist:
  - "NodeName={{ nodeGpuBase }} Name=gpu Type=teslak80 File=/dev/nvidia[0-7]"

# TIMERS
slurm_SlurmctldTimeout: "300"
slurm_SlurmdTimeout: "600"
slurm_InactiveLimit: "0"
slurm_MinJobAge: "30"
slurm_MessageTimeout: "30"
slurm_KillWait: "30"
slurm_CompleteWait: "12"
slurm_Waittime: "0"
slurm_KillOnBadExit: "1"
slurm_KeepAliveTime: "60"
slurm_OverTimeLimit: "60"

##
slurm_first_job_id: "2230000"
slurm_return_to_service: "1"
slurm_max_job_count: "30000"
slurm_max_job_time: "7-00:00:00"
slurm_job_def_time: "2:00:00"
slurm_max_array_size: "10001"

slurm_propagate_resource_limits_except: "MEMLOCK"
slurm_enforce_part_limits: "YES"

slurm_usepam: "1"
slurm_RebootProgram: "/sbin/reboot"

# If you wish to use job submit plugins (parameter JobSubmitPlugins=),
# uncomment and edit the following:
# slurm_jobsubmitplugins: "lua"

slurm_SchedulerType: "sched/backfill"
slurm_scheduler_parameters: "bf_max_job_test=300,bf_max_job_part=200,bf_max_job_user=30,defer,bf_continue,bf_window=7200,bf_resolution=1800,pack_serial_at_end"

slurm_SchedulerPort: "7321"
slurm_select_type: "select/cons_res"
slurm_select_type_parameters: "CR_Core_Memory"
slurm_fast_schedule: "2"
slurm_priority_type: "priority/multifactor"
slurm_priority_flags: "FAIR_TREE,SMALL_RELATIVE_TO_TIME,MAX_TRES"
slurm_priority_decayhalflife: "14-0"
slurm_priority_favorsmall: "NO"
slurm_priority_weight_fairshare: "10000000"
slurm_priority_weight_age: "10000"
slurm_priority_weight_partition: "0"
slurm_priority_weight_jobsize: "10000"
slurm_priority_weight_qos: "10000"
slurm_priority_maxage: "7"
# to not have any HealthCheckProgram set slurm_healthcheck_program: "" 
slurm_healthcheck_program: "/usr/sbin/nhc"
slurm_healthcheck_interval: "300"
slurm_cgroup_mountpoint: "/sys/fs/cgroup"
slurm_cgroup_automount : "no"
slurm_cgroup_constrain_cores: "yes"
slurm_cgroup_cpu_affinity: "yes"
slurm_cgroup_constrain_ram: "no" # Use the very strict cgroup-based memory tracker?
slurm_cgroup_allowedramspace: "100.1" # At which % of requested memory should the process get killed?
slurm_cgroup_constrain_swap: "no"
slurm_cgroup_taskaffinity: "yes" 
slurm_cgroup_allowedswapspace: "120" # % of memory allocation that can be phys mem+swap
slurm_cgroup_constrain_devices: "no" 
slurm_cgroup_min_ram: "500" # always allocate this much memory to each job
slurm_cgroup_release_agent_dir: "/etc/slurm/cgroup"

# LOGGING
slurm_SlurmctldDebug: "3"
slurm_SlurmdDebug: "3"
slurm_JobCompLoc: "{{ slurm_log_dir }}/slurm_jobcomp.log"
slurm_JobCompType: "jobcomp/filetxt"

# ACCOUNTING
slurm_JobAcctGatherType: "jobacct_gather/linux"
slurm_JobAcctGatherFrequency: "energy=60,task=60"
slurm_AcctGatherEnergyType: "acct_gather_energy/rapl"
slurm_AcctGatherNodeFreq: "60"

# Job-private /tmp, /var/tmp and /dev/shm
pam_use_namespace: False
pam_tmp_inst_dir: /l/tmp-inst/
pam_var_tmp_inst_dir: /l/vartmp_inst/

# template in a file to manage slurm logs via rsyslog
slurm_manage_rsyslog_conf: True
# sysctl settings from slurm htc guide.
slurm_manage_sysctl: True
slurm_sysctl_core_somaxconn: 4096 # 128 is the default in el7
slurm_sysctl_tcp_max_syn_backlog: 4096 # 512 is the default in el7

slurm_AccountingStorageTRES: "gres/gpu:teslak80"

#slurm_nodelist:
# - "NodeName=cn[1-1234] RealMemory=128000 Feature=haswell"
# - "NodeName=gpu[1-1234] RealMemory=128000 Feature=haswell"

#slurm_partitionlist:
# - "PartitionName=batch Nodes={{ slurm_compute_nodes }} Default=NO MaxTime=INFINITE State=UP DefaultTime=2:00:00"
# - "PartitionName=grid Nodes=gpu[1-1234] Default=NO MaxTime={{ slurm_max_job_time }} State=UP DefaultTime=2:00:00"

# To add extra parameters, uncomment the following line and the desired parameter's lines. Add any other absent but needed parameter to the list
#slurm_ExtraParamsList:
# - slurm_JobCredentialPrivateKey: ""
# - slurm_JobCredentialPublicCertificate: ""
# - slurm_PluginDir: ""
# - slurm_PlugStackConfig: ""
# - slurm_PropagatePrioProcess: ""
# - slurm_PropagateResourceLimits: ""
# - slurm_PrologSlurmctld: "/etc/slurm/slurmctld_prolog"
# - slurm_EpilogSlurmctld: "/etc/slurm/slurmctld_epilog"
# - slurm_SrunProlog: ""
# - slurm_SrunEpilog: ""
# - slurm_TaskProlog: ""
# - slurm_TrackWCKey: "no"
# - slurm_TreeWidth: "50"
# - slurm_TmpFs: ""
# - # set slurm_mysql_passwords to something. If this variable is not set the role will fail.
# - slurm_mysql_password: "CHANGEME"
# - slurm_version is only used when grabbing the source 
# - slurm_control_addr is the same as controlmachine by default. Define it here to change it.
# - slurm_control_addr: "{{ slurm_service_node }}"
# - # If you wish to add accounting for additional TRES types, uncomment and edit the following:
# - slurm_accounting_storage_tres: "gres/gpu:teslak80,gres/gpu:teslak40"
# - slurm_SchedulerAuth: ""
# - slurm_SchedulerRootFilter: ""
# - slurm_DefMemPerCPU: "512"
# - slurm_PriorityUsageResetPeriod: "14-0"
# - slurm_HealthCheckNodeState: "IDLE"
# - slurm_Licenses: "mdcs:256"
# - slurm_MaxSubmitJobs: "2000"
# - slurm_SlurmctldLogFile: {{ slurm_log_dir }}"/slurmctld.log"
# - slurm_DebugFlags: "backfill"
# - slurm_SlurmdLogFile: {{ slurm_log_dir }}"/slurmd.log"
# - slurm_JobAcctGatherFrequency: "task=30"
# - slurm_AccountingStoragePass: ""


# plugstack and X11
slurm_plugstack: False
slurm_x11_spank: False
slurm_x11_setting: "optional          x11.so"

slurm_spank_x11_packages:
  - xauth
  - slurm-spank-x11
