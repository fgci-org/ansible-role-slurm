---

# Defaults for ansible-role-slurm
#
slurm_packages:
 - slurm-plugins
 - slurm
 - slurm-perlapi
 - slurm-slurmdb-direct
 - slurm-sql
 - slurm-lua
 - slurm-devel
 - slurm-pam_slurm
 - slurm-sjstat
 - slurm-slurmdbd
 - slurm-torque
 - slurm-munge
 - slurm-sjobexit
# - slurm-spank-x11
#

slurm_node_packages:
 - slurm
 - slurm-devel
 - slurm-lua
 - slurm-munge
 - slurm-pam_slurm
 - slurm-perlapi
 - slurm-plugins
 - slurm-sjobexit
 - slurm-sjstat
 - slurm-sql
 - slurm-torque
# - slurm-spank-x11-0.2.5-1.x86_64
# - slurm-spank-x11-debuginfo-0.2.5-1.x86_64

slurm_dep_packages:
 - "xterm"
 - "sssd"
 - "gcc" 
 - "make"
 - "gcc-c++"
 - "wget"
 - "vim"
 - "man"
 - "rpm-build"
 - "pam"
 - "pam-devel"
 - "hwloc"
 - "hwloc-devel"
 - "munge"
 - "munge-devel"
 - "munge-libs"
 - "readline-devel"
 - "openssl-devel"
 - "perl-ExtUtils-MakeMaker"
 - "lua"
 - "lua-devel"
 - "lua-posix"
 - "lua-filesystem"

admingroup: "admin"
# By setting slurm_munge_key_from_nfs to True we copy the munge.key from slurm_munge_key_nfs
# If it's False then we copy it from files/ where ansible runs
# This is the default. So with ansible-pull we set slurm_munge_key_nfs to True
slurm_munge_key_from_nfs: False
# By default we copy the munge.key to NFS
slurm_munge_key_to_nfs: True
slurm_munge_key_nfs_dir: "/home/{{ admingroup }}"
slurm_munge_key_nfs: "{{ slurm_munge_key_nfs_dir }}/munge.key"
#

#####
fgci_install: True
fgci_slurmrepo_version: "fgcislurm1508"
siteName: "io"
nodeBase: "{{ siteName }}"
nodeGpuBase: "gpu"
#set slurm_mysql_passwords to something. If this variable is not set the role will fail.
#slurm_mysql_password: "CHANGEME"
# slurm_version is only used when grabbing the source 
slurm_version: "15.08.3"
# slurm_type (service, compute or submit) should be set with group_vars
slurm_type: ""
slurm_build: False
slurm_log_dir: "/var/log/slurm"
slurm_tmp_dir: "/opt/slurmdb/tmp/slurmd"
slurmd_tmp_dir: "{{ slurm_tmp_dir }}"
slurm_state_dir: "/opt/slurmdb/tmp"

slurm_switch_type: "switch/none"
slurm_mpi_default: "none"
slurm_mpi_params: "ports=12000-12999"
slurm_proctrack_type: "proctrack/cgroup"

# If the current node is a nis server, make sure the slurm user and
# group exist
nis_server: False

##
slurm_service_node: "{{ hostvars[groups['slurm_service'][0]]['ansible_hostname']  }}"
# slurm_control_addr is the same as controlmachine by default. Define it here to change it.
# slurm_control_addr: "{{ slurm_service_node }}"
slurm_accounting_storage_host: "{{ slurm_service_node }}"
slurm_accounting_storage_type: "accounting_storage/slurmdbd"
slurm_accounting_storage_loc: "slurm_acct_db"
slurm_accounting_storage_user: "slurm"
# If you wish to add accounting for additional TRES types, uncomment
# and edit the following:
#slurm_accounting_storage_tres: "gres/gpu:teslak80,gres/gpu:teslak40"
slurm_accounting_storage_enforce: "safe,qos"
# If you wish to use a topology plugins (parameter TopologyPlugin=),
# uncomment and edit the following:
#slurm_topology_plugin: "topology/tree"
slurm_clustername: "{{ siteName | default('test_cluster')}}"
slurm_compute_nodes: "{{ nodeBase }}[1-4]"
slurm_compute_realmemory: "126000"
slurm_compute_sockets: "2"
slurm_compute_corespersocket: "12"
slurm_compute_threadspercore: "2"
slurm_node_state: "UNKNOWN"
slurm_with_gpu: True
slurm_gpu_nodes: "{{ nodeGpuBase }}"
slurm_grestypes: "gpu"
slurm_greslist:
  - "NodeName={{ nodeGpuBase }} Name=gpu Type=teslak80 File=/dev/nvidia[0-7]"

##
slurm_first_job_id: "2230000"
slurm_return_to_service: "1"
slurm_max_job_count: "30000"
slurm_max_job_time: "7-00:00:00"
slurm_job_def_time: "2:00:00"
slurm_max_array_size: "10001"

slurm_propagate_resource_limits_except: "MEMLOCK"
slurm_enforce_part_limits: "YES"

slurm_usepam: "1"

# If you wish to use job submit plugins (parameter JobSubmitPlugins=),
# uncomment and edit the following:
# slurm_jobsubmitplugins: "lua"

slurm_scheduler_parameters: "bf_max_job_test=300,bf_max_job_part=200,bf_max_job_user=30,defer,bf_continue,bf_window=7200,bf_resolution=1800,pack_serial_at_end"
slurm_select_type: "select/cons_res"
slurm_select_type_parameters: "CR_Core_Memory"
slurm_fast_schedule: "2"
slurm_priority_type: "priority/multifactor"
slurm_priority_flags: "FAIR_TREE,SMALL_RELATIVE_TO_TIME,MAX_TRES"
slurm_priority_decayhalflife: "14-0"
slurm_priority_favorsmall: "NO"
slurm_priority_weight_fairshare: "10000000"
slurm_priority_weight_age: "10000"
slurm_priority_weight_partition: "0"
slurm_priority_weight_jobsize: "10000"
slurm_priority_weight_qos: "10000"
slurm_priority_maxage: "7"
slurm_healthcheck_program: "/usr/sbin/nhc"
slurm_healthcheck_interval: "300"
slurm_cgroup_mountpoint: "/sys/fs/cgroup"
slurm_cgroup_automount : "no"
slurm_cgroup_constrain_cores: "yes"
slurm_cgroup_cpu_affinity: "yes"
slurm_cgroup_constrain_ram: "no" # Use the very strict cgroup-based memory tracker?
slurm_cgroup_allowedramspace: "100.1" # At which % of requested memory should the process get killed?
slurm_cgroup_constrain_swap: "no"
slurm_cgroup_taskaffinity: "yes" 
slurm_cgroup_allowedswapspace: "120" # % of memory allocation that can be phys mem+swap
slurm_cgroup_constrain_devices: "no" 
slurm_cgroup_min_ram: "500" # always allocate this much memory to each job
slurm_cgroup_release_agent_dir: "/etc/slurm/cgroup"

# Job-private /tmp, /var/tmp and /dev/shm
pam_use_namespace: False
pam_tmp_inst_dir: /l/tmp-inst/
pam_var_tmp_inst_dir: /l/vartmp_inst/

# sysctl settings from slurm htc guide.
slurm_manage_sysctl: True
slurm_sysctl_core_somaxconn: 4096 # 128 is the default in el7
slurm_sysctl_tcp_max_syn_backlog: 4096 # 512 is the default in el7

#slurm_nodelist:
# - "NodeName=cn[1-1234] RealMemory=128000 Feature=haswell"
# - "NodeName=gpu[1-1234] RealMemory=128000 Feature=haswell"

#slurm_partitionlist:
# - "PartitionName=batch Nodes={{ slurm_compute_nodes }} Default=NO MaxTime=INFINITE State=UP DefaultTime=2:00:00"
# - "PartitionName=grid Nodes=gpu[1-1234] Default=NO MaxTime=INFINITE State=UP DefaultTime=2:00:00"
