# {{ ansible_managed }}
# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
#
ClusterName={{ slurm_clustername }}
ControlMachine={{ hostvars[groups['slurm_service'][0]]['ansible_hostname']  }}
ControlAddr={{ hostvars[groups['slurm_service'][0]]['ansible_hostname'] }}
#BackupController=service02
#BackupAddr=service02
#
SlurmUser=slurm
#SlurmdUser=root
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
StateSaveLocation={{ slurm_state_dir }}
SlurmdSpoolDir={{ slurm_tmp_dir }}
SwitchType=switch/none
MpiDefault=none
MpiParams=ports=12000-12999
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/linuxproc
#PluginDir=
CacheGroups=1
{% if slurm_first_job_id is defined %}
FirstJobId={{ slurm_first_job_id }}
{% else %}
FirstJobId=1
{% endif %}
ReturnToService=1
{% if slurm_max_job_count is defined %}
MaxJobCount={{ slurm_max_job_count }}
{% else %}
MaxJobCount=30000
{% endif %}
#PlugStackConfig=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
PropagateResourceLimitsExcept=MEMLOCK
EnforcePartLimits=YES
#Prolog=/etc/slurm/prolog
#Epilog=/etc/slurm/epilog
#PrologSlurmctld=/etc/slurm/slurmctld_prolog
#EpilogSlurmctld=/etc/slurm/slurmctld_epilog
#SrunProlog=
#SrunEpilog=
#TaskProlog=
TaskEpilog=/usr/bin/epilog
TaskPlugin=task/cgroup
#TaskPlugin=task/none
#TrackWCKey=no
#TreeWidth=50
#TmpFs=
UsePAM=1
RebootProgram=/sbin/reboot
#
HealthCheckInterval=300
HealthCheckProgram=/usr/bin/healthcheck
#HealthCheckNodeState=IDLE

#
#
#GresTypes=mic,gpu
#GresTypes=gpu
# TIMERS
SlurmctldTimeout=300
SlurmdTimeout=600
InactiveLimit=0
MinJobAge=30
MessageTimeout=30
KillWait=30
CompleteWait=12
Waittime=0
KillOnBadExit=1
KeepAliveTime=60
OverTimeLimit=60
#
# SCHEDULING
SchedulerType=sched/backfill
#SchedulerParameters     = bf_max_job_user=30,bf_continue,bf_interval=60,bf_resolution=180,max_job_bf=300,defer_rpc_cnt=10
{% if slurm_scheduler_parameters is defined %}
SchedulerParameters={{ slurm_scheduler_parameters }}
{% else %}
SchedulerParameters=bf_max_job_test=300,bf_max_job_part=200,bf_max_job_user=30,defer,bf_continue,bf_window=7200,bf_resolution=1800
{% endif %}
#
#SchedulerAuth=
SchedulerPort=7321
#SchedulerRootFilter=
SelectType=select/cons_res
{% if slurm_select_type_parameters is defined %}
SelectTypeParameters={{ slurm_select_type_parameters }}
{% else %}
SelectTypeParameters=CR_Core_Memory
{% endif %}
#DefMemPerCPU=512
FastSchedule=2
{% if slurm_priority_type is defined %}
PriorityType={{ slurm_priority_type }}
{% else %}
PriorityType=priority/multifactor
{% endif %}
{% if slurm_priority_decayhalflife is defined %}
PriorityDecayHalfLife={{ slurm_priority_decayhalflife }}
{% else %}
PriorityDecayHalfLife=14-0
{% endif %}
{% if slurm_priority_favorsmall is defined %}
PriorityFavorSmall={{ slurm_priority_favorsmall }}
{% else %}
PriorityFavorSmall=NO
{% endif %}
#PriorityUsageResetPeriod=14-0
{% if slurm_priority_weight_fairshare is defined %}
PriorityWeightFairshare={{Â slurm_priority_weight_fairshare }}
{% else %}
PriorityWeightFairshare=10000000
{% endif %}
{% if slurm_priority_weight_age is defined %}
PriorityWeightAge={{ slurm_priority_weight_age }}
{% else %}
PriorityWeightAge=10000
{% endif %}
{% if slurm_priority_weight_partition is defined %}
PriorityWeightPartition={{ slurm_priority_weight_partition }}
{% else %}
PriorityWeightPartition=0
{% endif %}
{% if slurm_priority_weight_jobsize is defined %}
PriorityWeightJobSize={{ slurm_priority_weight_jobsize }}
{% else %}
PriorityWeightJobSize=10000
{% endif %}
{% if slurm_priority_weight_qos is defined %}
PriorityWeightQOS={{ slurm_priority_weight_qos }}
{% else %}
PriorityWeightQOS=10000
{% endif %}
{% if slurm_priority_maxage is defined %}
PriorityMaxAge={{ slurm_priority_maxage }}
{% else %}
PriorityMaxAge=7-0
{% endif %}
#Licenses=mdcs:256
#MaxSubmitJobs=2000
#
# LOGGING
SlurmctldDebug=3
#SlurmctldLogFile={{ slurm_log_dir }}/slurmctld.log
SlurmdDebug=3
#DebugFlags=backfill
#SlurmdLogFile={{ slurm_log_dir }}/slurmd.log
JobCompLoc={{ slurm_log_dir }}/slurm_jobcomp.log
JobCompType=jobcomp/filetxt
#
# ACCOUNTING
JobAcctGatherType=jobacct_gather/linux
JobAcctGatherFrequency=energy=60,task=60
AcctGatherEnergyType=acct_gather_energy/rapl
AcctGatherNodeFreq=60
#JobAcctGatherFrequency=task=30
#

#
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost={{ hostvars[groups['slurm_service'][0]]['ansible_hostname']  }}
AccountingStorageLoc=slurm_acct_db
#AccountingStoragePass=
AccountingStorageUser=slurm
{% if slurm_accounting_storage_enforce is defined %}
AccountingStorageEnforce={{ slurm_accounting_storage_enforce }}
{% else %}
AccountingStorageEnforce=limits,qos
{% endif %}


#JobSubmitPlugins=lua

#
# TOPOLOGY
#
#TopologyPlugin=topology/tree
# COMPUTE NODES
#NodeName={{ nodeBase }}[1-4] RealMemory=126000 Sockets=2 CoresPerSocket=6 ThreadsPerCore=1 State=UNKNOWN
NodeName={{ slurm_compute_nodes }} RealMemory=126000 Sockets=2 CoresPerSocket=6 ThreadsPerCore=1 State=UNKNOWN

{% if slurm_with_gpu %}
NodeName={{ slurm_gpu_nodes }} RealMemory=250000 Sockets=2 CoresPerSocket=6 ThreadsPerCore=1 State=UNKNOWN feature=gpu
PartitionName=normal Nodes={{ slurm_compute_nodes }},{{ slurm_gpu_nodes }} Default=YES MaxTime=INFINITE State=UP DefaultTime=2:00:00
{% else %}
PartitionName=normal Nodes={{ slurm_compute_nodes }} Default=YES MaxTime=INFINITE State=UP DefaultTime=2:00:00
{% endif %}
# partitions

PartitionName=test Nodes={{ slurm_compute_nodes }} Default=NO MaxTime=INFINITE State=UP DefaultTime=2:00:00
PartitionName=grid Nodes={{ slurm_compute_nodes }} Default=NO MaxTime=INFINITE State=UP DefaultTime=2:00:00


